{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6b5ccf-4cf5-47b0-bd84-f323c226d814",
   "metadata": {},
   "source": [
    "# Reproducible Deep Learning Experiment: Land Use Classification with EuroSAT\n",
    "\n",
    "This notebook demonstrates a fully reproducible deep learning pipeline using the EuroSAT dataset—a benchmark of Sentinel-2 satellite images for land use classification introduced by Helber et al. (2019) [^1]. Our goal is to present an end-to-end workflow covering data acquisition, preprocessing, model training, and evaluation, all encapsulated within a reproducible containerized environment. By following best practices such as version control and Infrastructure as Code, this demo project enables others to easily replicate and extend the experiment.\n",
    "\n",
    "![EuroSAT Dataset Overview](https://raw.githubusercontent.com/phelber/EuroSAT/refs/heads/master/eurosat_overview_small.jpg)\n",
    "\n",
    "[^1]: P. Helber, B. Bischke, A. Dengel and D. Borth, \"EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification,\" in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 12, no. 7, pp. 2217-2226, July 2019, doi: 10.1109/JSTARS.2019.2918242.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ffc466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import EuroSAT\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# set a seed for the experiment's repeatability, thus ACM's R1\n",
    "RANDOM_SEED = 23\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Enforce deterministic behavior in cuDNN\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# change directory\n",
    "if not os.path.exists(\"Finetune_ResNet.ipynb\"):\n",
    "    os.chdir(\"src\")\n",
    "assert os.path.exists(\"Finetune_ResNet.ipynb\")\n",
    "\n",
    "# Check if cuda can access the GPU via CUDA drivers\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Select device '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764edf2-75f6-451c-a69e-7abf9af44b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for saving results\n",
    "RESULT_FILE = \"ResNet_results.csv\"\n",
    "MODEL_FILE = \"model.pt\"\n",
    "PATH_DATA = \"../data\"\n",
    "\n",
    "# hyperparameters\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f49782-78be-4820-80b2-adf64c3be6e0",
   "metadata": {},
   "source": [
    "## Load a pretrained ResNet model\n",
    "\n",
    "Init a ResNet18 model with pre-trained weights using PyTorch’s `torchvision.models` library.\n",
    "See the [PyTorch documentation on initializing pretrained models](https://pytorch.org/vision/main/models.html#initializing-pre-trained-models) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebec137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b62f08-7dcc-47ac-b701-428ff4a792b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52c94f87-d093-4c40-bbdc-9b4e833a0de8",
   "metadata": {},
   "source": [
    "## Adapt the Final Layer and Freeze all but the last layers\n",
    "\n",
    "\n",
    "The last fully connected layer of ResNet18 is designed to output 1,000 classes by default (as it is trained on ImageNet). Since EuroSAT has only 10 classes, we need to replace this layer.\n",
    "To retain the general feature-extracting capabilities of the pre-trained network, we freeze the weights of the earlier layers. This prevents them from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the final linear layer (fc)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 10)  # EuroSAT has 10 classes\n",
    "\n",
    "# freeze all but the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only the final fc layer’s parameters will be updated\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf1e54-f16e-4e21-a6eb-674e4af252c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d67bd38e-3abb-47f0-87c9-2e47e524ad24",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Load the EuroSAT dataset using `torchvision.datasets` with an appropriate transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and use source-specific mean and std for normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize to model's input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create a context that disables certificate verification to allow downloading the dataset\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Load the EuroSAT dataset\n",
    "dataset = EuroSAT(root=PATH_DATA, transform=transform, download=True)\n",
    "\n",
    "# It's highly recommended to re-enable certificate verification after downloading\n",
    "ssl._create_default_https_context = ssl.create_default_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590ea8f-7d68-467f-9773-46461ab7141b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9af42ed8-7965-49a7-883f-819afedfb925",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Here, we want to use only 10 % of all training instances to decrease the time for computation. \n",
    "Additionally, the dataset is split into 80 % training and 20 % validation instances. The number of images for each dataset is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9997a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use 10 % of the dataset to decrease the runtime here\n",
    "dataset_sub = torch.utils.data.Subset(\n",
    "    dataset, np.random.permutation(len(dataset))[:int(len(dataset)*0.1)]\n",
    ")\n",
    "\n",
    "# Split dataset into 80 % training and 20 % validation images\n",
    "train_size = int(0.8 * len(dataset_sub))\n",
    "val_size = len(dataset_sub) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset_sub, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_size, shuffle=False)\n",
    "\n",
    "print(f\"Sizes of train/val datasets: {len(train_dataset)} / {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b841981-7c25-4619-951d-1d6c9b81d7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce26d559-24e9-46f8-b255-9b1c7afc46e4",
   "metadata": {},
   "source": [
    "## Define Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c23ab6-c6ca-4ddd-93d2-70d5e08eab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "def train_and_validate(n_epochs, model, train_loader, val_loader, optimizer, criterion, verbose=1):\n",
    "    \"\"\"\n",
    "    Train and validate a neural network model over a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        n_epochs (int): Number of epochs for training.\n",
    "        model (torch.nn.Module): The neural network model to train.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        criterion (torch.nn.Module): Loss function to compute training loss.\n",
    "        verbose (int): Verbosity level; higher numbers print more details.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The trained model and two lists containing training and validation accuracies per epoch.\n",
    "    \"\"\"\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    # move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # aggregate statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        train_acc = 100.0 * correct_train / total_train  # in percentage\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        val_acc = 100.0 * correct_val / total_val  # in percentage\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        if verbose >= 1:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{n_epochs}], \"\n",
    "                f\"Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "                f\"Training Accuracy: {train_acc:.2f} %, \"\n",
    "                f\"Validation Accuracy: {val_acc:.2f} %\"\n",
    "            )\n",
    "\n",
    "    return model, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a3c06-7473-48f3-b248-3f4568c3d3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67156696-ba0f-4a33-9473-949b56fee62c",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00674c5e-a131-4b5f-8d00-e6caa444d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate the model\n",
    "model, train_accs, val_accs = train_and_validate(\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    optimizer=optimizer, criterion=criterion,\n",
    "    model=model, n_epochs=N_EPOCHS, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The epoch-wise results to reproduce are listed below:\n",
    "# Epoch [1/10], Loss: 2.1624, Training Accuracy: 22.04%, Validation Accuracy: 48.89%\n",
    "# Epoch [2/10], Loss: 1.5579, Training Accuracy: 57.73%, Validation Accuracy: 68.15%\n",
    "# Epoch [3/10], Loss: 1.1514, Training Accuracy: 74.95%, Validation Accuracy: 76.11%\n",
    "# Epoch [4/10], Loss: 0.9031, Training Accuracy: 81.85%, Validation Accuracy: 79.63%\n",
    "# Epoch [5/10], Loss: 0.7532, Training Accuracy: 84.81%, Validation Accuracy: 82.78%\n",
    "# Epoch [6/10], Loss: 0.6467, Training Accuracy: 86.48%, Validation Accuracy: 83.70%\n",
    "# Epoch [7/10], Loss: 0.5804, Training Accuracy: 87.36%, Validation Accuracy: 84.63%\n",
    "# Epoch [8/10], Loss: 0.5212, Training Accuracy: 88.47%, Validation Accuracy: 84.81%\n",
    "# Epoch [9/10], Loss: 0.4857, Training Accuracy: 88.43%, Validation Accuracy: 86.48%\n",
    "# Epoch [10/10], Loss: 0.4472, Training Accuracy: 89.72%, Validation Accuracy: 86.11%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db2c56-8967-47c3-83d7-f50385e2f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to the specified file\n",
    "result_df = pd.DataFrame(\n",
    "    {\"train_accs\": train_accs, \"val_accs\": val_accs},\n",
    "    index=np.arange(1, N_EPOCHS + 1)\n",
    ")\n",
    "result_df.to_csv(RESULT_FILE, index_label=\"Epoch\")\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9bdf8-429e-40da-acea-d217d591f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save PyTorch model\n",
    "torch.save(model.to(\"cpu\"), MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4b729-a9e1-43e9-a192-3755cf7894bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d4711b-46d6-4652-8672-5eeec4f036b3",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Plot the training progress and a confusion matrix of our model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89529b73-683d-48a4-a5f0-ac757c809481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lines\n",
    "sns.lineplot(result_df[\"train_accs\"], color=\"firebrick\", marker=\"D\", linestyle=\"-\", label=\"Training Accuracy\")\n",
    "sns.lineplot(result_df[\"val_accs\"], color=\"steelblue\", marker=\"D\", linestyle=\"-\", label=\"Training Accuracy\")\n",
    "\n",
    "# Customise plot\n",
    "plt.title(\"Accuracy Across Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(None, 100)\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae55331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all classes of the validation dataset\n",
    "val_true = list()\n",
    "val_preds = list()\n",
    "_ = model.eval()  # set model to evaluation mode (important when using e.g. dropouts)\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        val_true += list(labels.numpy())\n",
    "        outputs = model.to(device)(images.to(device))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        val_preds += list(predicted.to(\"cpu\").numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344071d8-f089-40c6-8977-0e222302bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix and normalize it to percentages\n",
    "conf_matrix = confusion_matrix(val_true, val_preds, normalize=\"true\") * 100  # Normalized to show percentages\n",
    "\n",
    "# Define class names based on EuroSAT's 10 classes\n",
    "class_names = dataset.classes  # EuroSAT class labels\n",
    "\n",
    "# Plot the confusion matrix with percentage values using seaborn\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\".1f\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "\n",
    "# Customizing axis and titles\n",
    "plt.title(\"Confusion Matrix for EuroSAT Classification (Percentages)\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a43fdd-4982-4dd7-85c6-301846db73a5",
   "metadata": {},
   "source": [
    "The confusion matrix is dominated by its diagonal entries, reflecting high overall classification accuracy.\n",
    "The most frequent misclassification occurred when permanent highways were incorrectly identified as AnnualCrop, accounting for 7.8 % of all highways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7050295-4e7c-4f90-a5cd-c3fbd4dc1170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
